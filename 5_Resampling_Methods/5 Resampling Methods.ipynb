{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqUR/TFdEUMAyhFQQ62+4F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Resampling\n","\n","Think of resampling like taste-testing food before serving a big party:\n","\n","1. You make small samples from your big batch\n","\n","2. You taste each sample to see if it's good\n","\n","3. This helps you predict how the entire batch will taste\n","\n","In statistics: **Resampling means repeatedly drawing samples from your data to understand how your model performs.**\n","\n","## Why do we need Resampling\n","\n","- Withoug Resampling:\n","1. Wit fit model once on all data\n","2. We have no idea how it performs on new data\n","\n","- With Resampling:\n","1. We simulate having new data by splitting what we have\n","2. We get multiple performance estimates\n","3. We understand how reliable our model is\n","\n","### Simple Problem\n","\n","When we train a model, we usually measure **training error**.\n","\n","But what we actually care about is **test error**:\n","\n","> How well will this model perform on *new data*?\n","\n","#### Key Issue\n","\n","* Training error is **optimistic** (too good)\n","* Test error is **unknown** (we donâ€™t have infinite new data)\n","\n","\n","### Solution\n","\n","Use **resampling methods**:\n","\n","* Repeatedly split data\n","* Train models\n","* Measure errors on data not used for training\n","\n","---\n","\n","#### Training Error vs Test Error\n","\n","| Type           | Meaning                             |\n","| -------------- | ----------------------------------- |\n","| Training Error | Error on data used to fit the model |\n","| Test Error     | Error on new, unseen data           |\n","\n","> Training error almost always **underestimates** test error.\n","\n","---\n"],"metadata":{"id":"i_VQ8abR3DF9"}},{"cell_type":"markdown","source":["Two main types we'll learn:\n","\n","1. Cross-Validation: \"Test-driving\" your model multiple times\n","\n","2. Bootstrap: \"Copying and shuffling\" your data to measure uncertainty\n","\n","## Model Assessment vs. Model Selection\n","\n","Before diving into the methods, it is important to distinguish between two different goals:\n","\n","1. **Model Assessment**: The process of evaluating how well a specific model performs. We want to estimate the test error (how the model performs on data it has never seen before).\n","\n","2. **Model Selection**: The process of choosing the right level of \"flexibility\" or complexity for a model (e.g., choosing between a simple straight line or a complex curved line).\n","\n"],"metadata":{"id":"f8_0rKyY2PLp"}},{"cell_type":"markdown","source":["#Cross-Validation"],"metadata":{"id":"MJAUpxffcAeD"}},{"cell_type":"markdown","source":["## 1. Validation Set Approach\n","\n","### 1.1 What It Is\n","\n","This is the simplest form of cross-validation. You take your data and split it into two piles:\n","\n","* **Training set** (used to fit the model)\n","* **Validation set** (used to evaluate performance)\n","\n","### 1.2 Workflow\n","\n","1. Split data (e.g. 50% / 50%)\n","2. Train model on training set\n","3. Compute error on validation set\n","\n","\n","### 1.3 Error Metric (Regression)\n","\n","$$\n","\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","$$\n","\n","**Where:**\n","\n","* $y_i$ = true value\n","* $\\hat{y}_i$ = predicted value\n","* $n$ = number of validation observations\n","\n","---\n","\n","### 1.4 Example (Intuition)\n","\n","Predict car mileage using horsepower:\n","\n","* Linear model â†’ underfits\n","* Quadratic model â†’ better\n","* Cubic model â†’ no improvement\n","\n","Validation MSE helps us choose the **right model complexity**.\n","\n","---\n","\n","### 1.5 Drawbacks\n","\n","1. **High variability** (depends on random split): The test error estimate can change drastically depending on which specific observations end up in the training set versus the validation set.\n","\n","2. **Less training data** (Overestimating Error) $\\rightarrow$ worse models: Since the model is trained on fewer observations than the full dataset, it might perform worse than a model trained on everything. This often leads to an \"overestimate\" of the true test error.\n","\n","---\n"],"metadata":{"id":"NsgSAIKD2ltJ"}},{"cell_type":"markdown","source":["## 2. Leave-One-Out Cross-Validation (LOOCV)\n","\n","LOOCV attempts to fix the drawbacks of the simple validation split.\n","### 2.1 Core Idea\n","\n","If you have $n$ observations, you train the model on $n-1$ data points and test it on the single point you left out. You repeat this $n$ times, leaving out a different data point each time.\n","\n","\n","---\n","\n","### 2.2 LOOCV Error Formula\n","\n","The LOOCV estimate for the test Mean Squared Error (MSE) is the average of all these individual test errors\n","\n","$$\n","\\text{CV}(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{MSE}_i\n","$$\n","\n","**Where:**\n","\n","* $n$: The total number of observations in  dataset.\n","* $i$: The current iteration (from 1 to $n$).\n","* $MSE_i$: The Mean Squared Error calculated for the $i^{th}$ observation that was left out, calculated as $(y_i - \\hat{y}_i)^2$.\n","* $CV_{(n)}$: The final estimated test error.\n","\n","\n","---\n","\n","### 2.3 Why LOOCV Is Better Than Validation Set\n","\n","| Aspect        | Validation Set | LOOCV    |\n","| ------------- | -------------- | -------- |\n","| Training size | ~50%           | $n-1$    |\n","| Bias          | High           | Very low |\n","| Randomness    | Yes            | No       |\n","\n","---\n","\n","### 2.4 Computational Shortcut (Linear Regression Only)\n","\n","For least squares regression:\n","\n","$$\n","\\text{CV}(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\n","$$\n","\n","**Where:**\n","\n","* $\\hat{y}_i$ = fitted value from full model\n","* $h_i$ = leverage of observation $i$ (how influential it is)\n","\n","> High-leverage points get **penalized more**, which corrects bias.\n","\n","---\n","\n","### 2.5 Limitation of LOOCV\n","\n","* High Variance. Because the training sets are 99% identical, the outputs are highly correlated, which can make the average estimate less stable.\n","* Computational Cost. Normally, you have to fit the model $n$ times, which is slow for large data.\n","\n","---\n"],"metadata":{"id":"hIlUsl7e2uB7"}},{"cell_type":"markdown","source":["\n","## 3. k-Fold Cross-Validation\n","\n","This is the \"Goldilocks\" methodâ€”the middle ground that most professionals use.\n","\n","### 3.1 Core Idea\n","\n","1. Split data into **k equal-sized folds**\n","2. Use 1 fold as validation, remaining as training\n","3. Repeat k times\n","4. Average the errors\n","\n","---\n","\n","### 3.2 k-Fold CV Formula\n","\n","$$\n","\\text{CV}(k) = \\frac{1}{k} \\sum_{i=1}^{k} \\text{MSE}_i\n","$$\n","\n","**Where:**\n","\n","* $k$ = number of folds\n","* $\\text{MSE}_i$ = the error computed on the $i^{th}$ group that was held out.\n","\n","---\n","\n","### 3.3 Typical Choices of k\n","\n","| k  | Comment           |\n","| -- | ----------------- |\n","| 5  | Good balance      |\n","| 10 | Industry standard |\n","| n  | LOOCV             |\n","\n","---\n","\n"],"metadata":{"id":"XtE9Gp_H2zmv"}},{"cell_type":"markdown","source":["## 3.4. Biasâ€“Variance Trade-off in Cross-Validation\n","\n","### Bias Perspective\n","\n","k-fold has slightly more bias than LOOCV because it uses less data for training (e.g., in 5-fold, it uses 80% of the data).\n","\n","* Validation set â†’ **high bias**\n","* k-fold â†’ **moderate bias**\n","* LOOCV â†’ **low bias**\n","\n","### Variance Perspective\n","\n","k-fold has lower variance than LOOCV. This is because the training sets in k-fold have less overlap, making the results of the $k$ models less correlated.\n","\n","* LOOCV â†’ **high variance** (models are highly correlated)\n","* k-fold â†’ **lower variance**\n","\n","### Final Recommendation\n","\n","> Use **5-fold or 10-fold CV** in practice. As it provides a test error estimate that isn't too biased but is also very stable (low variance).\n","\n","---\n"],"metadata":{"id":"zK4QNh4M21rA"}},{"cell_type":"markdown","source":["\n","## 4. Cross-Validation for Classification\n","\n","So far, we used MSE for regression (predicting a number like \"Charges\"). But what if we are predicting a category (like \"Smoker\" or \"Non-Smoker\")?\n","\n","In classification, we don't use MSE. Instead, we use the Error Rate (the number of misclassified observations).\n","\n","### Key Change\n","\n","* Regression â†’ use **MSE**\n","* Classification â†’ use **error rate (misclassification error)**\n","\n","**For instance, in the classification setting, the LOOCV error rate takes the form**:\n","\n","$$\n","\\text{CV}(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\text{Err}_i\n","$$\n","\n","**Where:**\n","\n","* $\\text{Err}_i = I(y_i \\neq \\hat{y}_i)$\n","* $I(\\cdot)$ = indicator function (1 if true, 0 otherwise)\n","That is $Err_i$  an indicator ($0$ or $1$). It is $1$ if the model predicted the wrong category for observation $i$, and $0$ if it was correct\n","\n","---\n","\n","### Key Insight for Selection\n","\n","When we plot the Training Error vs. the Cross-Validation Error, the Training Error usually keeps going down as the model gets more complex (flexibility). However, the CV Error usually forms a U-shape. The bottom of that \"U\" tells us the optimal level of complexity to avoid overfitting.\n"],"metadata":{"id":"2k6ZifbTH-YL"}},{"cell_type":"markdown","source":["## **Logistic Regression & Increasing Complexity**\n","\n","In real-world data science, we often don't know if the relationship between our variables is a simple straight line or a complex curve. We use **Polynomials** to add flexibility to our models.\n","\n","### **1. What is \"Increasing Complexity\"?**\n","\n","Think of it like learning to recognize animals:\n","\n","* Simple model: \"If it has 4 legs, it's a dog\" (linear boundary)\n","\n","* Medium complexity: \"If it has 4 legs AND barks, it's a dog\" (curved boundary)\n","\n","* High complexity: \"If it has 4 legs, barks, wags tail, eats bones...\" (very complex boundary)\n","\n","Complexity refers to how \"wiggly\" or flexible the model is.\n","\n","* **Low Complexity (Degree 1):** A straight line (Linear). It might be too simple to catch the trend (**Underfitting**).\n","* **Medium Complexity (Degree 2 or 3):** A smooth curve (Quadratic/Cubic).\n","* **High Complexity (Degree 10+):** A very wiggly line that tries to touch every single data point (**Overfitting**). It learns the \"noise\" of the data rather than the actual pattern.\n","\n","---\n","\n","### **2. The Quadratic Logistic Regression Formula**\n","\n","As complexity increases, we add squared or cubed versions of our predictors (X) to the equation. For example, a quadratic logistic regression looks like this:\n","\n","$$log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_1^2 + \\beta_3X_2 + \\beta_4X_2^2$$\n","\n","**Variable Explanation:**\n","\n","* p: The probability of an event happening (e.g., passing the exam).\n","\n","\n","* $log\\left(\\frac{p}{1-p}\\right)$: The **Log-Odds** (or Logit), which is the standard output for logistic regression.\n","\n","\n","* $X_1, X_2$: Your original predictor variables (e.g., Age or BMI).\n","\n","\n","* $X_1^2, X_2^2$: The \"complexity\" terms that allow the decision boundary to be **curved**.\n","\n","\n","* $\\beta$: The coefficients the model learns to weigh each variable.\n","\n","\n","\n","---\n","\n","### **3. Using 10-Fold CV to Choose Complexity**\n","\n","How do we know which degree to use? We look at the **Error Rate Curves**.\n","\n","* **Training Error (Blue Line):** This almost always goes **down** as complexity increases because the model \"memorizes\" the training data better and better.\n","\n","\n","* **Test/CV Error (Black/Brown Line):** This typically forms a **U-Shape**.\n","\n","\n","* At first, the error drops as the model gets better.\n","* After a certain point, the error starts to go back **up** because the model is now overfitting.\n","\n","\n","\n","\n","\n","**The Goal:** We pick the degree where the **10-fold CV error is at its lowest point**. Even if the training error is lower elsewhere, the CV error is a much better guess for how the model will perform on new data.\n","\n","\n","## Practical Takeaways (Exam + Interview Ready)\n","\n","* Training error is misleading\n","* Validation set is simple but unstable\n","* LOOCV is unbiased but expensive\n","* 5- or 10-fold CV is the best practical choice\n","* CV works for **both regression and classification**\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"LD--nplASThG"}},{"cell_type":"markdown","source":["#Bootstrap\n","\n","\n","##What Is the Bootstrap?\n","\n","**Bootstrap** is a statistical technique used to **measure uncertainty**.\n","\n","In simple terms:\n","\n","> The bootstrap tells us **how reliable or unstable our estimate is**\n","> (for example: a mean, a regression coefficient, or a model parameter).\n","\n","It answers questions like:\n","\n","* *How much could this estimate change if I collected new data?*\n","* *Is this number stable or noisy?*\n","\n","##Why Do We Need the Bootstrap?\n","\n","In real life:\n","\n","* We usually have **only one dataset**\n","* We **cannot repeatedly collect new samples** from the population\n","\n","But to measure uncertainty, we *wish* we had many datasets.\n","\n","ðŸ‘‰ **Bootstrap solves this problem by reusing the same dataset intelligently.**\n","\n","##Sampling â€œWith Replacementâ€ (Very Important)\n","\n","### What does *with replacement* mean?\n","\n","When we create a bootstrap sample:\n","\n","* We randomly pick observations\n","* **After picking one, we put it back**\n","* So the same observation **can appear multiple times**\n","\n","### Example\n","\n","Original data ($n = 5$):\n","\n","```\n","[2, 4, 6, 8, 10]\n","```\n","\n","One bootstrap sample:\n","\n","```\n","[4, 4, 10, 2, 8]\n","```\n","\n","Another bootstrap sample:\n","\n","```\n","[6, 6, 6, 8, 2]\n","```\n","\n","Each bootstrap sample:\n","\n","* Has the **same size ($n$)**\n","* Contains **duplicates**\n","* Misses some original points\n","\n","##Bootstrap Algorithm (Step-by-Step)\n","\n","Assume:\n","\n","* Original dataset = ($Z$)\n","* Sample size = ($n$)\n","\n","### Step 1: Create Bootstrap Samples\n","\n","* Randomly sample ($n$) observations **with replacement**\n","* Call this new dataset ($Z_1^*$)\n","\n","### Step 2: Compute the Estimate\n","\n","* Calculate the statistic of interest\n","  (mean, coefficient, alpha, accuracy, etc.)\n","* Call it ($\\hat{\\theta}_1^*$)\n","\n","### Step 3: Repeat\n","\n","* Repeat steps 1 and 2 **B times** (usually 1000 or more)\n","\n","You now have:\n","$$\n","\\hat{\\theta}_1^*, \\hat{\\theta}_2^*, \\dots, \\hat{\\theta}_B^*\n","$$\n","\n","---\n","\n","##What Do We Get from Bootstrap?\n","\n","From these repeated estimates, we can compute:\n","\n","### Standard Error (Most Common Use)\n","\n","**Bootstrap Standard Error:**\n","\n","$$\n","SE_{boot}(\\hat{\\theta}) =\n","\\sqrt{\n","\\frac{1}{B-1}\n","\\sum_{r=1}^{B}\n","\\left(\n","\\hat{\\theta}_r^* - \\bar{\\hat{\\theta}}^*\n","\\right)^2\n","}\n","$$\n","\n","### Meaning of Symbols\n","\n","* $B$: number of bootstrap samples created\n","* $\\hat{\\theta}_r^*$: The estimate (like mean or a slope) calculated from the $r^{th}$ bootstrap dataset\n","* $\\bar{\\hat{\\theta}}^*$: average of all bootstrap estimates\n","\n","ðŸ‘‰ This tells us **how much the estimate varies**.\n","\n","\n","---\n","\n","##What the Bootstrap Is Used For\n","\n","Bootstrap is especially useful when:\n","\n","* Standard formulas for uncertainty **do not exist**\n","* Models are complex\n","* Data is limited\n","\n","### Common Applications\n","\n","* Standard error of regression coefficients\n","* Confidence intervals\n","* Stability of ML models\n","* Medians, percentiles, custom statistics\n","\n","---\n","\n","##hat Bootstrap Is NOT Used For\n","\n","âŒ It is **not** mainly for:\n","\n","* Model selection\n","* Estimating test error\n","\n","ðŸ‘‰ That is the role of **cross-validation**\n","\n","---\n","\n","## ðŸ”Ÿ One-Sentence Summary (Exam-Ready)\n","\n","> **Bootstrap estimates uncertainty by repeatedly resampling the original dataset with replacement and measuring how much the estimate varies.**\n","\n"],"metadata":{"id":"4CADEWSHZemo"}}]}