{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **MULTIPLE LINEAR REGRESSION**\n",
        "---\n",
        "\n",
        "# #Ô∏è‚É£ **Why Do We Need Multiple Linear Regression?**\n",
        "\n",
        "In real life, an outcome (sales, price, marks, etc.) usually depends on **more than one factor**.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Sales depend on **TV + Radio + Newspaper** ads.\n",
        "* House price depends on **size + location + age + rooms**.\n",
        "* Student marks depend on **study hours + sleep + tuition + motivation**.\n",
        "\n",
        "If we use separate simple linear regressions for each factor, we get confusing results because:\n",
        "\n",
        "1. **We cannot make one combined prediction**.\n",
        "2. **Predictors may be correlated with each other**, causing misleading coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "# #Ô∏è‚É£ **The Multiple Linear Regression Model**\n",
        "\n",
        "Multiple Linear Regression allows us to predict a response ($Y$) based on multiple predictors ($X_1, X_2, ..., X_p$).\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\varepsilon\n",
        "$$\n",
        "\n",
        "Instead of a line, we are now fitting a plane (in 3D) or a hyperplane (in higher dimensions).\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "| Symbol        | Meaning                                                           |\n",
        "| ------------- | ----------------------------------------------------------------- |\n",
        "| (Y)           | Response/outcome (e.g., sales)                                    |\n",
        "| $(X_j)$         | j-th predictor (e.g., TV budget)                                  |\n",
        "| $(\\beta_0)$     | Intercept                                                         |\n",
        "| $(\\beta_j)$     | Effect of predictor $(X_j)$, **holding all other predictors fixed** |\n",
        "| $(\\varepsilon)$ | Random error                                                      |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚≠ê Interpretation of $(\\beta_j)$ (Very Important!)\n",
        "\n",
        "$\n",
        "\\beta_j = \\text{Change in } Y \\text{ from a 1-unit increase in } X_j \\text{ while keeping all other variables constant.}\n",
        "$\n",
        "\n",
        "Example (Advertising):\n",
        "\n",
        "* $(\\beta_{\\text{radio}} = 0.189)$ means:\n",
        "\n",
        "> ‚ÄúIf you increase radio advertising by $1,000 **while keeping TV and newspaper fixed**, sales increase by about 189 units.‚Äù\n",
        "\n",
        "This interpretation is fundamentally different from simple regression.\n",
        "\n",
        "---\n",
        "\n",
        "# #Ô∏è‚É£ ** Estimating Coefficients (Least Squares)**\n",
        "\n",
        "The prediction for any observation is:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n",
        "$$\n",
        "\n",
        "Least squares chooses $(\\beta_0, \\dots, \\beta_p)$ that minimize:\n",
        "\n",
        "$$\n",
        "RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $(y_i)$ = actual value\n",
        "* $(\\hat{y}_i)$ = predicted value\n",
        "\n",
        "In multiple regression, the formulas involve matrices, so we rely on software.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# #Ô∏è‚É£ **The \"Shark Attack\" Analogy**\n",
        "\n",
        "Imagine a dataset showing a strong positive relationship between Ice Cream Sales and Shark Attacks.\n",
        "\n",
        "* Does eating ice cream cause shark attacks? No.\n",
        "* The Hidden Variable: Temperature.\n",
        "* Hot days $\\rightarrow$ More people buy ice cream.\n",
        "* Hot days $\\rightarrow$ More people swim $\\rightarrow$ More shark attacks.\n",
        "* If you run a multiple regression with both Ice Cream and Temperature, the coefficient for Ice Cream would drop to zero.\n",
        "---\n",
        "\n",
        "# **Important Questions Multiple Regression Answers**\n",
        "\n",
        "These are:\n",
        "\n",
        "1. **Is at least one predictor useful?**\n",
        "2. **Which predictors matter?**\n",
        "3. **How well does the model fit the data?**\n",
        "4. **How accurate are our predictions?**\n",
        "\n",
        "---\n",
        "\n",
        "#  **Question 1: Is There Any Relationship between the predictors and the responses? (F-Test)**\n",
        "\n",
        "We don't just look at individual p-values anymore. We test if at least one predictor is useful.\n",
        "\n",
        "We test:\n",
        "\n",
        "* Null hypothesis: $H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$  (All coefficients are zero; the model is useless)\n",
        "\n",
        "* Alternative: $H_a: \\text{At least one } \\beta_j \\neq 0$\n",
        "\n",
        "### The Test: F-Statistic Formula:\n",
        "\n",
        "Logic: The F-statistic compares the explained variance (TSS - RSS) to the unexplained variance (RSS).\n",
        "\n",
        "$$\n",
        "F =\n",
        "\\frac{(TSS - RSS)/p}\n",
        "{RSS/(n - p - 1)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "| Symbol | Meaning                 |\n",
        "| ------ | ----------------------- |\n",
        "| (TSS)  | Total variability in Y  |\n",
        "| (RSS)  | Unexplained variability |\n",
        "| (n)    | Number of observations  |\n",
        "| (p)    | Number of predictors    |\n",
        "\n",
        "* If **F ‚âà 1**, predictors are useless (no relationship).\n",
        "* If **F ‚â´ 1**, at least one predictor matters.\n",
        "\n",
        "**Why not just check every individual p-value?** If you have 100 predictors, simple chance says about 5 of them will look \"significant\" (p < 0.05) even if they are random noise. The F-statistic tests the **whole team** at once, protecting you from these false positives.\n",
        "\n",
        "---\n",
        "\n",
        "# #Ô∏è‚É£ **9. Testing a Subset of Predictors**\n",
        "\n",
        "Sometimes we test only q predictors.\n",
        "\n",
        "Suppose we test whether last q predictors have zero coefficients.\n",
        "\n",
        "Second model $RSS = (RSS_0)$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "F =\n",
        "\\frac{(RSS_0 - RSS)/q}\n",
        "{RSS/(n - p - 1)}\n",
        "$$\n",
        "\n",
        "This tells whether removing predictors significantly worsens the model.\n",
        "\n",
        "---\n",
        "\n",
        "# #Ô∏è‚É£ **Question 2: Deciding Which Variables Matter (Variable Selection)**\n",
        "\n",
        "We often want to trim the \"dead weight\" predictors. Since checking every possible combination of variables is computationally impossible (for 30 variables, there are >1 billion combinations!), we use shortcuts:\n",
        "Three classical methods:\n",
        "\n",
        "---\n",
        "\n",
        "## **A. Forward Selection**\n",
        "\n",
        "Start with **no** predictors, then:\n",
        "\n",
        "1. Start with a blank model and add the single best predictor that reduces RSS the most\n",
        "2. Then add the  next best\n",
        "3. Stop when nothing else helps. (Good for when variables > samples) (based on rule like p-value or AIC)\n",
        "\n",
        "---\n",
        "\n",
        "## **B. Backward Selection**\n",
        "\n",
        "Start with **all** predictors, then:\n",
        "\n",
        "1. Remove the least significant predictor (whith highest p-value)\n",
        "2. Repeat until all remaining predictors are significant\n",
        "\n",
        "Cannot use if **p > n** (Only works if samples > variables).\n",
        "\n",
        "---\n",
        "\n",
        "## **C. Stepwise (Mixed) Selection**\n",
        "\n",
        "Combination:\n",
        "\n",
        "* Add predictors like forward\n",
        "* Remove predictors whose p-value becomes high\n",
        "\n",
        "More flexible and commonly used.\n",
        "\n",
        "---\n",
        "\n",
        "# **Question 3: How Well Does the Model Fit?**\n",
        "We use two main metrics:\n",
        "\n",
        "1. $R^2$ (Coefficient of Determination)**\n",
        "\n",
        "$$R^2 = Cor(Y, \\hat{Y})^2$$\n",
        "\n",
        "* It measures the proportion of variance.\n",
        "* **$R^2$ always increases** when adding more predictors (even useless ones!). A tiny increase in $R^2$ (like adding Newspaper) suggests the variable isn't actually helpful. Therefore, a small increase in $R^2$ is meaningless\n",
        "\n",
        " 2. **RSE (Residual Standard Error)**\n",
        "\n",
        " $$RSE = \\sqrt{\\frac{RSS}{n - p - 1}}$$\n",
        "\n",
        "* It measures the average standard deviation of the error (how far off our predictions are).\n",
        "\n",
        "* Unlike $R^2$, RSE can actually get worse (increase) if you add a useless variable, because the penalty for complexity ($p$) increases (because denominator = (n - p - 1))\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Question 4: How accurate are our predictions?**\n",
        "\n",
        "Prediction formula:\n",
        "\n",
        "$$\n",
        "\\hat{Y} = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p\n",
        "$$\n",
        "\n",
        "Three types of uncertainty:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Coefficient Uncertainty ‚Üí Confidence Interval**\n",
        "\n",
        "* Predicts the average response for a given set of $X$ values.\n",
        "\n",
        "* Example: \"If we spend $100k on TV, the average sales across all such markets will be between 10,985 and 11,528.\"\n",
        "\n",
        "This addresses:\n",
        "\n",
        "> How close is our estimate of the **average** response?\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Model Bias**\n",
        "\n",
        "Linear model may be oversimplified.\n",
        "We pretend model is correct for now.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Random Error ‚Üí Prediction Interval**\n",
        "\n",
        "* Predicts the response for a single specific value (e.g., one specific city next month).\n",
        "\n",
        "* Example: \"For this specific market, sales will be between 7,930 and 14,580.\"\n",
        "\n",
        "* Note: Prediction intervals are always wider because they account for both the uncertainty in the model coefficients (reducible error) AND the random noise in that one specific data point (irreducible error)\n",
        "\n",
        "Addresses:\n",
        "\n",
        "> How much will a **new individual observation** vary?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Example 1: House Price Prediction**\n",
        "\n",
        "Predictors:\n",
        "\n",
        "* Size\n",
        "* Number of bedrooms\n",
        "* Distance to city center\n",
        "\n",
        "Model:\n",
        "\n",
        "$$\n",
        "\\text{Price} = \\beta_0 + \\beta_1(\\text{Size}) + \\beta_2(\\text{Bedrooms}) + \\beta_3(\\text{Distance}) + \\varepsilon\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* $(\\beta_3 < 0)$: farther houses are cheaper\n",
        "* $(\\beta_2)$: effect of adding an extra bedroom **holding size constant**\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Example 2: Student Marks**\n",
        "\n",
        "Predictors:\n",
        "\n",
        "* Study hours\n",
        "* Sleep hours\n",
        "* Attendance\n",
        "\n",
        "Multiple regression separates:\n",
        "\n",
        "* True effect of study\n",
        "* True effect of sleep\n",
        "* True effect of attendance\n",
        "\n",
        "Even if students who study more also attend more, regression isolates each effect.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå **Example 3: Salary Prediction**\n",
        "\n",
        "Predictors:\n",
        "\n",
        "* Experience\n",
        "* Education\n",
        "* Number of certifications\n",
        "\n",
        "simple regression (salary vs certifications) may be misleading because certifications and experience are correlated.\n",
        "\n",
        "Multiple regression fixes this.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2ddorRVR06Ut"
      }
    }
  ]
}