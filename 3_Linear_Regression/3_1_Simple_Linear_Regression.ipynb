{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Part 1: Introduction to Linear Regression**\n",
        "\n",
        "### **What is Linear Regression?**\n",
        "\n",
        "Linear regression is a supervised learning method used to predict a **quantitative response** (a number) based on a **predictor** (input data).\n",
        "\n",
        "Think of it as finding the \"best fit\" straight line through a cloud of data points. It is one of the oldest and most widely used statistical learning methods.\n",
        "\n",
        "### **Why use it?**\n",
        "\n",
        "Even though it is simple, it serves as the foundation for modern machine learning approaches. It helps us answer critical questions like:\n",
        "\n",
        "1. **Is there a relationship?**\n",
        "   (e.g., Does advertising spending actually increase sales?)\n",
        "\n",
        "2. **How strong is the relationship?**\n",
        "   (e.g., Does a huge budget boost sales a lot, or just a little?)\n",
        "\n",
        "3. **Prediction:**\n",
        "   (e.g., If we spend $50k on TV ads, how many units will we sell?)\n",
        "\n",
        "### **Real-World Analogy: Taxi Fare**\n",
        "\n",
        "Imagine predicting taxi fare ($Y$) based on distance traveled ($X$):\n",
        "\n",
        "$$ Y ≈ \\beta_0 + \\beta_1X + \\epsilon $$\n",
        "\n",
        "\n",
        "| Term                      | Meaning                             |\n",
        "| ------------------------- | ----------------------------------- |\n",
        "| **Intercept ($\\beta_0$)** | Base fare when you sit in the taxi. |\n",
        "| **Slope ($\\beta_1$)**     | Cost per kilometer.                 |\n",
        "| **Error ($\\epsilon$)**    | Traffic delays, route changes, etc. |\n",
        "\n",
        "---\n",
        "\n",
        "# **Part 2: Simple Linear Regression**\n",
        "\n",
        "Simple linear regression predicts a response $Y$ based on a **single** predictor variable $X$. We assume there is a straight-line relationship.\n",
        "\n",
        "### **The Formula**\n",
        "\n",
        "$$Y \\approx \\beta_0 + \\beta_1 X$$\n",
        "\n",
        "### **Variable Breakdown**\n",
        "\n",
        "| Symbol        | Name               | Explanation                                                   |\n",
        "| :------------ | :----------------- | :------------------------------------------------------------ |\n",
        "| **$Y$**       | Response Variable  | The output you want to predict (e.g., Sales).                 |\n",
        "| **$X$**       | Predictor Variable | The input you have (e.g., TV Ad Budget).                      |\n",
        "| **$\\beta_0$** | Intercept          | The value of $Y$ when $X = 0$.                                |\n",
        "| **$\\beta_1$** | Slope              | The average increase in $Y$ for every 1-unit increase in $X$. |\n",
        "| **$\\approx$** | Approximation      | Means \"is approximately modeled as.\"                          |\n",
        "\n",
        "### **Prediction Equation**\n",
        "\n",
        "In the real world, we don't know the true $\\beta_0$ and $\\beta_1$. We have to estimate them using data. Once we have estimates (denoted by \"hats\" $\\hat{}$), we can make a prediction\n",
        "\n",
        "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* **$\\hat{y}$**: Predicted value\n",
        "* **$\\hat{\\beta}_0$**: Estimated intercept\n",
        "* **$\\hat{\\beta}_1$**: Estimated slope\n",
        "\n",
        "---\n",
        "\n",
        "# **Part 3: Estimating the Coefficients (Training the Model)**\n",
        "\n",
        "How do we find the best $\\beta_0$ and $\\beta_1$? We find the best $\\beta_0$ and $\\beta_1$ by fitting the line that is closest to the actual data.\n",
        "\n",
        "### **Residual ($e_i$)**\n",
        "\n",
        "The difference between the **actual value** ($y_i$) and the **predicted value** ($\\hat{y}_i$) is called the residual:\n",
        "\n",
        "$$e_i = y_i - \\hat{y}_i$$\n",
        "\n",
        "If $e_i$ is positive, our model under-predicted. If negative, it over-predicted.\n",
        "* If $e_i > 0$: Model under-predicted.\n",
        "* If $e_i < 0$: Model over-predicted.\n",
        "\n",
        "### **Residual Sum of Squares (RSS)**\n",
        "\n",
        "To measure how \"bad\" our model is, we square all the residuals (to get rid of negatives) and add them up. This is the Residual Sum of Squares.\n",
        "\n",
        "$$RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "Expanded form:\n",
        "\n",
        "$$RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2$$\n",
        "\n",
        "### **Least Squares Method**\n",
        "\n",
        "Linear regression chooses $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that make RSS **as small as possible**.\n",
        "\n",
        "Formulas:\n",
        "\n",
        "$$\\hat{\\beta}*1 = \\frac{\\sum*{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
        "\n",
        "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* **$\\bar{y}$** = average of all $y$ values\n",
        "* **$\\bar{x}$** = average of all $x$ values\n",
        "\n",
        "---\n",
        "\n",
        "# **Part 4: Assessing the Accuracy of Coefficients**\n",
        "\n",
        "We have estimates ($\\hat{\\beta}$), but how much can we trust them?\n",
        "\n",
        "### **True vs Estimated Relationship**\n",
        "\n",
        "* **Population Regression Line:**\n",
        "  The actual relationship in the world\n",
        "  $$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n",
        "\n",
        "* **Least Squares Line:**\n",
        "  The relationship we estimate from our sample.\n",
        "\n",
        "**Bias Concept:**\n",
        "If we repeatedly sampled new datasets, the *average* of all estimated lines would equal the true line. This means linear regression estimates are **unbiased**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Standard Error (SE)**\n",
        "\n",
        "The Standard Error tells us how far our estimate ($\\hat{\\beta}_1$) is likely to be from the actual value ($\\beta_1$) on average.\n",
        "\n",
        "$$SE(\\hat{\\beta}*1)^2 = \\frac{\\sigma^2}{\\sum*{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* **$\\sigma^2$** = variance of the error term (noise)\n",
        "\n",
        "**Key Insight:** SE decreases if:\n",
        "\n",
        "* We have more data\n",
        "* The $x$ values are more spread out\n",
        "\n",
        "### **Confidence Intervals (CI)**\n",
        "\n",
        "A 95% Confidence Interval is a range of values defined so that, if we repeated the experiment over and over, 95% of those intervals would contain the true unknown parameter3333.\n",
        "\n",
        "Think of it like fishing:\n",
        "\n",
        "Point Estimate ($\\hat{\\beta}_1$): Throwing a spear at a specific spot. You might miss the fish (the true value) slightly.\n",
        "\n",
        "Confidence Interval: Casting a net. You are 95% sure the fish is somewhere inside the net.\n",
        "\n",
        "**The Formula**:\n",
        "\n",
        "For Linear Regression, the 95% confidence interval is approximately:\n",
        "\n",
        "$$\\text{Lower Limit} = \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1)$$\n",
        "\n",
        "$$\\text{Upper Limit} = \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1)$$\n",
        "\n",
        "Or written simply:\n",
        "\n",
        "$$\\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1)$$\n",
        "\n",
        "* $\\hat{\\beta}_1$: Your estimated slope.\n",
        "\n",
        "* $SE(\\hat{\\beta}_1)$: The Standard Error (how much the estimate wiggles on average).\n",
        "\n",
        "* Note: The number '2' is an approximation. Precisely, it depends on the \"t-distribution\", but for large datasets, it is very close to 25.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Hypothesis Testing (Courtroom Analogy)**\n",
        "\n",
        "We use Hypothesis Tests to decide if there is actually a relationship between $X$ and $Y$.\n",
        "\n",
        "#### **Null Hypothesis ($H_0$):**\n",
        "\n",
        "No relationship ($\\beta_1 = 0$)\n",
        "\n",
        "#### **Alternative Hypothesis ($H_a$):**\n",
        "\n",
        "There is a relationship ($\\beta_1 \\neq 0$)\n",
        "\n",
        "To test this, compute the **t-statistic**:\n",
        "\n",
        "$$t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* **t-statistic**: Measures how many standard deviations our coefficient is away from zero. **Large |t|** → strong evidence against $H_0$\n",
        "* **p-value**: The probability of seeing this result by pure luck if there was no relationship. **Small p-value (< 0.05)**: It is very unlikely to be luck. We reject the null hypothesis and conclude a relationship exists.\n",
        "\n",
        "---\n",
        "\n",
        "# **Part 5: Assessing the Accuracy of the Model**\n",
        "\n",
        "We know the variables are related, but how good is the model at predicting?\n",
        "\n",
        "### **1. Residual Standard Error (RSE)**\n",
        "\n",
        "RSE measures, on average, how far predictions are from the regression line. It is the average amount that the response (Y) deviates from the true regression line. It is a measure of the \"lack of fit\" in absolute units (e.g., if predicting sales, RSE is in \"units sold\").\n",
        "\n",
        "$$RSE = \\sqrt{\\frac{1}{n-2} RSS}$$\n",
        "\n",
        "Interpretation example:\n",
        "If RSE = 3.26, predictions are typically off by about 3.26 units.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. $R^2$ Statistic (R-Squared)**\n",
        "\n",
        "Since RSE depends on units (dollars, meters, liters), it's hard to tell if it's \"good.\"$R^2$ tells us what fraction of the variation in $Y$ is explained by the model, ranging from 0 to 1.\n",
        "\n",
        "$$R^2 = 1 - \\frac{RSS}{TSS}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* **TSS** = total variation in $Y$ (data) before regression\n",
        "* **RSS** = variation left unexplained by the model\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* **$R^2 = 1$** — The model explains 100% of the variability (perfect model).\n",
        "* **$R^2 = 0.61$** — model explains 61% of variation (Decent fit)\n",
        "* **$R^2 = 0$** — model explains nothing\n",
        "\n",
        "### **Correlation vs $R^2$**\n",
        "\n",
        "n Simple Linear Regression, $R^2$ is actually just the correlation ($r$) squared:\n",
        "\n",
        "$$R^2 = (Cor(X, Y))^2$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "oqLk6900AXZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vi3VEcvEAm0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VjwoEyjjAgyr"
      }
    }
  ]
}